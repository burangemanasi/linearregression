{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear Regression & GLMs â€” Student Lab\n",
        "\n",
        "Complete all TODOs. Avoid sklearn for core parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 â€” Synthetic Dataset (with collinearity)\n",
        "We generate data where features can be highly correlated to motivate ridge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n",
            "corr(x0,x1)= 0.9985704465455699\n"
          ]
        }
      ],
      "source": [
        "def make_regression(n=400, d=5, noise=0.5, collinear=True):\n",
        "    X = rng.standard_normal((n, d))\n",
        "    if collinear and d >= 2:\n",
        "        X[:, 1] = X[:, 0] * 0.95 + 0.05 * rng.standard_normal(n)\n",
        "    w_true = rng.standard_normal(d)\n",
        "    y = X @ w_true + noise * rng.standard_normal(n)\n",
        "    return X, y, w_true\n",
        "\n",
        "X, y, w_true = make_regression()\n",
        "n, d = X.shape\n",
        "check('shapes', y.shape == (n,))\n",
        "print('corr(x0,x1)=', np.corrcoef(X[:,0], X[:,1])[0,1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 â€” OLS Closed Form\n",
        "\n",
        "### Task 1.1: Closed-form w_hat using solve\n",
        "\n",
        "# TODO: compute w_hat using solve on (X^T X) w = X^T y\n",
        "# HINT: `XtX = X.T@X`, `Xty = X.T@y`, `np.linalg.solve(XtX, Xty)`\n",
        "\n",
        "**Checkpoint:** Why is explicit inverse discouraged?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Explicit inversion is avoided because it is less stable, slower, and more error-prone than solving the linear system directly.\n",
        "# Less Stable: Computing inverse magnifies floating point rounding errors, Slower: Matrix inversion computes entire matrix inverse, np.linalg.solve() avoids computing full inverse and only computes the solution which is faster and more efficient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: w_shape\n"
          ]
        }
      ],
      "source": [
        "XtX = X.T @ X                           # Compute feature covariance matrix used in normal equation\n",
        "Xty = X.T @ y                           # Compute featureâ€“target correlation vector\n",
        "w_hat = np.linalg.solve(XtX, Xty)       # Solve normal equation to obtain OLS weight estimates\n",
        "\n",
        "check('w_shape', w_hat.shape == (d,))   # Verify learned weight vector has correct dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Evaluate fit + residuals\n",
        "Compute:\n",
        "- predictions y_pred\n",
        "- MSE\n",
        "- residual mean and std\n",
        "\n",
        "**Interview Angle:** What does a structured residual pattern imply (e.g., nonlinearity)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A structured residual pattern means the model is biased because it cannot represent the true data-generating process. Linear regression assumes linearity, independent errors, constant variance, zero mean residuals.\n",
        "# If residuals are not random, the model is determined to be wrong. It's either too simple, missing features or violating assumptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mse 0.2289243168902079 resid_mean 0.029301285107637246 resid_std 0.47756230125633753\n",
            "OK: finite\n"
          ]
        }
      ],
      "source": [
        "y_pred = X @ w_hat                          # Compute model predictions using learned OLS weights\n",
        "mse = float(np.mean((y_pred - y) ** 2))     # Calculate mean squared error between predictions and targets\n",
        "resid = y_pred - y                          # Compute residuals (prediction errors)\n",
        "print('mse', mse, 'resid_mean', resid.mean(), 'resid_std', resid.std()) \n",
        "check('finite', np.isfinite(mse))           # Ensure computed loss is a valid finite number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 â€” Gradient Descent\n",
        "\n",
        "### Task 2.1: Implement MSE loss + gradient\n",
        "\n",
        "Loss = mean((Xw-y)^2), grad = (2/n) X^T(Xw-y)\n",
        "\n",
        "# TODO: implement `mse_loss_and_grad`\n",
        "\n",
        "**FAANG gotcha:** shapes and constants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: grad_shape\n",
            "OK: finite_loss\n"
          ]
        }
      ],
      "source": [
        "def mse_loss_and_grad(X, y, w):             # Compute MSE loss and its gradient for linear regression\n",
        "    r = X @ w - y                           # Compute MSE loss and its gradient for linear regression\n",
        "    loss = float(np.mean(r * r))            # Compute mean squared error loss\n",
        "    grad = (2.0 / X.shape[0]) * (X.T @ r)   # Compute gradient via chain rule\n",
        "    return loss, grad\n",
        "\n",
        "w0 = np.zeros(d)\n",
        "loss0, g0 = mse_loss_and_grad(X, y, w0)     # Evaluate loss and gradient at initial weights\n",
        "check('grad_shape', g0.shape == (d,))\n",
        "check('finite_loss', np.isfinite(loss0))    # Ensure loss value is finite and valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Train with GD + compare to closed-form\n",
        "\n",
        "# TODO: implement a simple GD loop, track loss, and compare final weights to w_hat.\n",
        "\n",
        "**Checkpoint:** How does feature scaling affect GD?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature scaling dramatically improves Gradient Descent because it reshapes the loss surface into a well-conditioned problem, while closed-form solutions donâ€™t depend on convergence speed but still benefit from better numerical stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "final_loss 0.23136626716013964\n",
            "||w_gd-w_hat|| 1.376975427170424\n",
            "OK: loss_decreases\n"
          ]
        }
      ],
      "source": [
        "def train_gd(X, y, lr=0.05, steps=500):             # train linear model using gradient descent\n",
        "    w = np.zeros(X.shape[1])                        # initialize weight vector to zeros\n",
        "    losses = []                                     # store loss values across iterations\n",
        "    for _ in range(steps):                          # iterate gradient descent updates\n",
        "        loss, g = mse_loss_and_grad(X, y, w)        # compute current loss and gradient\n",
        "        losses.append(loss)                         # track loss to monitor convergence\n",
        "        w = w - lr * g                              # update weights in negative gradient direction\n",
        "    return w, losses                                # return weights and loss history\n",
        "\n",
        "w_gd, losses = train_gd(X, y, lr=0.05, steps=500)   # gradient descent to learn regression weights\n",
        "print('final_loss', losses[-1])\n",
        "print('||w_gd-w_hat||', np.linalg.norm(w_gd - w_hat))   # Compare gradient descent solution with closed-form OLS solution\n",
        "check('loss_decreases', losses[-1] <= losses[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 â€” Ridge Regression (L2)\n",
        "\n",
        "### Task 3.1: Ridge closed-form\n",
        "w = (X^T X + Î»I)^{-1} X^T y\n",
        "\n",
        "# TODO: implement ridge_solve\n",
        "\n",
        "**Interview Angle:** Why does ridge help under collinearity?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ridge helps under collinearity because adding ðœ†ð¼ (lam) increases small eigenvalues (Ridge pushes all eigenvalues away from zero), stabilizes matrix inversion (Matrix is no longer near-singular), reduces variance (Adds small bias), and prevents coefficient explosion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: ridge_shape\n"
          ]
        }
      ],
      "source": [
        "def ridge_solve(X, y, lam):\n",
        "    d = X.shape[1]                  # number of features\n",
        "    return np.linalg.solve(         \n",
        "        X.T @ X + lam * np.eye(d),  # adds L2 regularization for stability\n",
        "        X.T @ y)                    # correlation between features and targets\n",
        "\n",
        "\n",
        "w_ridge = ridge_solve(X, y, lam=1.0)        # compute ridge-regularized weight vector\n",
        "check('ridge_shape', w_ridge.shape == (d,)) # ensure output has one weight per feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Bias/variance demo with train/test split\n",
        "\n",
        "# TODO: split into train/test and compare MSE for multiple lambdas.\n",
        "\n",
        "**Checkpoint:** why can test error improve even when train error worsens?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test error improves while train error worsens because regularization reduces variance more than it increases bias, improving generalization. Training error always prefers complexity. Test error prefers the right balance. You trade a little bias for a big reduction in variance when lam is added."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lam, train_mse, test_mse\n",
            "(0.0, np.float64(0.21799841512243526), np.float64(0.257745393681019))\n",
            "(0.1, np.float64(0.21809342046539712), np.float64(0.2584161606858411))\n",
            "(1.0, np.float64(0.219110917219492), np.float64(0.2610634849213479))\n",
            "(10.0, np.float64(0.22348087752854684), np.float64(0.2703372913154806))\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "idx = rng.permutation(n)\n",
        "train = idx[: int(0.7*n)]\n",
        "test = idx[int(0.7*n):]\n",
        "Xtr, ytr = X[train], y[train]\n",
        "Xte, yte = X[test], y[test]\n",
        "\n",
        "lams = [0.0, 0.1, 1.0, 10.0]\n",
        "results = []\n",
        "for lam in lams:\n",
        "    w = ridge_solve(Xtr, ytr, lam=lam) if lam > 0 else np.linalg.solve(Xtr.T@Xtr, Xtr.T@ytr)\n",
        "    tr_mse = np.mean((Xtr@w - ytr)**2)\n",
        "    te_mse = np.mean((Xte@w - yte)**2)\n",
        "    results.append((lam, tr_mse, te_mse))\n",
        "print('lam, train_mse, test_mse')\n",
        "for r in results:\n",
        "    print(r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 â€” GLM Intuition\n",
        "\n",
        "### Task 4.1: Match tasks to (distribution, link)\n",
        "Fill in a table for:\n",
        "- regression\n",
        "- binary classification\n",
        "- count prediction\n",
        "\n",
        "**Explain:** what changes when you go from OLS to a GLM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Problem | Target type | Distribution | Link | Loss |\n",
        "|---|---|---|---|---|\n",
        "| House price | continuous | Gaussian (Normal) | Identity | MSE (Squared error) |\n",
        "| Fraud | binary | Bernoulli | Sigmoid | Log Loss (cross-entropy) |\n",
        "| Clicks per user | count | Poisson | Log | Poisson Deviance |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GLM lets you choose distribution based on target type:\n",
        "# Gaussian â†’ continuous\n",
        "# Bernoulli â†’ binary\n",
        "# Poisson â†’ counts\n",
        "# So the noise model changes.\n",
        "# GLMs generalize OLS by allowing different distributions and link functions, making linear modeling work for binary and count data, not just continuous targets. GLM extends linear modeling to Probabilities (Fraud detection), counts (clicks, events), rates and many real world problems while OLS works for continuous outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Train/test results shown for ridge\n",
        "- Short answers to checkpoint questions\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
